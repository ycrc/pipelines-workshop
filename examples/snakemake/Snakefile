# To run this pipeline:
#   cd examples/snakemake
#   module load snakemake
#   snakemake -n          # dry run — shows what would be executed
#   snakemake -j1         # run with 1 core
#   snakemake -j4         # run with 4 cores in parallel
#   snakemake --dag | dot -Tpng > dag.png   # visualize the DAG

from itertools import combinations

# Path to input data relative to this Snakefile's directory
DATA_DIR = "../data"

# Discover play names from input files
PLAYS, = glob_wildcards(DATA_DIR + "/{play}.txt")

# Generate all unique pairs (sorted so play1 < play2 alphabetically)
PAIRS = list(combinations(sorted(PLAYS), 2))
PLAY1S = [p[0] for p in PAIRS]
PLAY2S = [p[1] for p in PAIRS]

# Constrain wildcards: play names use lowercase letters and hyphens only.
# This prevents ambiguity when parsing {play1}_{play2} filenames.
wildcard_constraints:
    play  = "[a-z-]+",
    play1 = "[a-z-]+",
    play2 = "[a-z-]+"

# Use bash (not sh) so we can use process substitution <(...)
shell.executable("/bin/bash")

# Default target: when this is complete, the pipeline is complete.
rule all:
    input:
        "output/similarity_matrix.csv"

# Step 1a: Clean text — lowercase, remove punctuation, one word per line
rule clean_text:
    input:
        DATA_DIR + "/{play}.txt"
    output:
        temp("output/{play}.clean.txt")
    shell:
        """
        cat {input} \
            | tr '[:upper:]' '[:lower:]' \
            | tr -d '[:punct:]' \
            | tr -s '[:space:]' '\\n' \
            > {output}
        """

# Step 1b: Count word frequencies
rule count_words:
    input:
        "output/{play}.clean.txt"
    output:
        temp("output/{play}.counts.txt")
    shell:
        """
        sort {input} | uniq -c | sort -rn > {output}
        """

# Step 1c: Extract top 100 most frequent words
rule top_words:
    input:
        "output/{play}.counts.txt"
    output:
        "output/{play}.top100.txt"
    shell:
        """
        head -100 {input} > {output}
        """

# Step 2: Compare two plays using Jaccard similarity of their top words
rule compare_plays:
    input:
        top1 = "output/{play1}.top100.txt",
        top2 = "output/{play2}.top100.txt"
    output:
        "output/{play1}_{play2}.similarity"
    shell:
        """
        COMMON=$(comm -12 \
            <(awk '{{print $2}}' {input.top1} | sort) \
            <(awk '{{print $2}}' {input.top2} | sort) \
            | wc -l)
        TOTAL1=$(wc -l < {input.top1})
        TOTAL2=$(wc -l < {input.top2})
        UNION=$((TOTAL1 + TOTAL2 - COMMON))
        echo "scale=3; $COMMON / $UNION" | bc > {output}
        """

# Step 3: Combine all pairwise results into a CSV matrix
rule combine_results:
    input:
        expand("output/{play1}_{play2}.similarity", zip, play1=PLAY1S, play2=PLAY2S)
    output:
        "output/similarity_matrix.csv"
    shell:
        """
        echo "play1,play2,similarity" > {output}
        for file in {input}; do
            basename=$(basename "$file" .similarity)
            play1=$(echo "$basename" | cut -d'_' -f1)
            play2=$(echo "$basename" | cut -d'_' -f2-)
            similarity=$(cat "$file")
            echo "${{play1}},${{play2}},${{similarity}}" >> {output}
        done
        """
